Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?
Revisión de artículo.
El artículo fue publicado en octubre del 2014 y escrito en el 2013, por manuel Fernández-Delgado, Eva Cernadas, Senén Barro y Dinani Anorim, investigadores de las universidades de Santiago de Compostela (España) y del Estado de Bahía (Brasil).
Este artículo propone una revisión de 179 algorítmos de Machine Learning de clasificación aplicado a 121 bases de datos de acceso público, con el fin de determinar si estadísticamente existe un algorítmo que sea mejor que todos para, generalmente, cualquier base de datos. Sin embargo, también se hace la claridad en dicho artículo que, según el teorema de No-Free-Lunch (NFL), públicado por Wolpert en 1996; no es posible generalizar un modelo de Machine Learning que sea el mejor para todas las bases de datos.

En el artículo mencionan que existen algunas situaciones que sesgan el compotamiento del modelo, así como la elección, tanto del modelo, como de los parámetros. Para evitar esto, o por lo menos disminuir su efecto, los autores realizaron una seríe de aclaraciones y de presunciones, que las enumeran en 5 criterios:
1. Usar bases de datos que fueran suficientemente grandes para evitar que pocos datos invalidaran las conclusiones. La fuente utilizada fue bses de datos libres UCI.
2. Para no entrar en favoritismos por ningun modelo, se asumen que todos son correctos y no requieren ningún ajuste. También se utilizan técnicas para refinar los parámetros en la mayoría de los casos (donde es posible) o se toman los refinamientos dados por los mismos aplicativos usados, con el fin de compensar el desconocimiento de algunos algorítmos respecto a otros, por la experiencia de los autores.
3. Aun no es posible obtener la máxima precisión con ningún algoritmo, así que se asume que la mayor precisión obtenida es la mayor que se puede obtener, en el momento del estudio. Esto no quiere decir que no surja un nuevo algorítmo con mejores rendimientos en el futuro.
4. No es posible determinar si una baja medida de precisión (rendimiento) de algún algoritmo se debe a una limitación del algoritmo en sí o de los datos, así que se asume que se debe a las limitaciones del propio algorimto, ya que otros algorítmos pueden obtener mejores rendimientos.
5. La partición de los datos en entrenamiento y pruebas, puede estar sesgado, para corregir esto, se usa la misma partición para todos los algorítmos estudiados y además, se hacen publicas estas particiones para que el experimento sea replcable por cualquiera.

El estudio encontró que, para las bases de datos usadas, el algorítmo Random Forest Paralelo es el que mejor precisión obtuvo (la más alta: 94,1%) y el que obtuvo mejores precisiones para la mayoría de las bases de datos (mejor precisión en 102 de 121 bases de datos). A este modelo le realizaron ajuste en el parámetro mtry (en R con el paquete caret). Le sigue el Random Forest, con un poco menor precisión máxima, pero con un promedio ligeramente mayor. Seis algorítmos de Random Forest y 5 de SVM (Máquinas de Soporte Vectorial), son los que mejores precisiones obtuvieron en el top 20 de clasificadores.

Este estudio por tanto, plantea una interesante conclusión, en la que en general, los mejores resultados se obtendrán con calsificadores de la familia Random Forest y SVM, principalmente, se espera que el mejor modelo se haría con el Random Forest Paralelo.

